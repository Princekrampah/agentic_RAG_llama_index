{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [\n",
    "    \"./datasets/lora_paper.pdf\",\n",
    "    \"./datasets/longlora_efficient_fine_tuning.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ./datasets/lora_paper.pdf paper tool.\n",
      "Creating ./datasets/longlora_efficient_fine_tuning.pdf paper tool.\n"
     ]
    }
   ],
   "source": [
    "from utils import create_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "\n",
    "for paper in papers:\n",
    "    print(f\"Creating {paper} paper tool.\")\n",
    "    path = Path(paper)\n",
    "    vector_tool, summary_tool = await create_doc_tools(doc_name=path.stem, document_fp=path)\n",
    "    paper_to_tools_dict[path.stem] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lora_paper': [<llama_index.core.tools.query_engine.QueryEngineTool at 0x7864e89c5b10>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7864e89c5d80>],\n",
       " 'longlora_efficient_fine_tuning': [<llama_index.core.tools.query_engine.QueryEngineTool at 0x7864e868e050>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7864e868de10>]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_to_tools_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.core.tools.query_engine.QueryEngineTool object at 0x7864e89c5b10>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7864e89c5d80>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7864e868e050>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7864e868de10>]\n"
     ]
    }
   ],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[Path(paper).stem]]\n",
    "print(str(initial_tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Agent Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain to me what is Lora and why it's being used.Explain to me what is LongLoRA and why it's being used.Compare and contract LongLoRA and Lora.\n",
      "=== Calling Function ===\n",
      "Calling function: lora_paper_summary_query_engine_tool with args: {\"input\": \"Explain what is Lora and why it's being used.\"}\n",
      "=== Function Output ===\n",
      "LoRA, or Low-Rank Adaptation, is a method utilized in deep learning to adapt large-scale pre-trained language models to specific tasks or domains. It involves introducing trainable rank decomposition matrices into each layer of the Transformer architecture while freezing the pre-trained model weights. This approach significantly reduces the number of trainable parameters for downstream tasks, making it more memory and computationally efficient. LoRA is being used to address the challenges associated with full fine-tuning, particularly for models with a large number of parameters like GPT-3, making it more feasible and cost-effective to adapt these models to various tasks without compromising model quality.\n",
      "=== Calling Function ===\n",
      "Calling function: longlora_efficient_fine_tuning_summary_query_engine_tool with args: {\"input\": \"Explain what is LongLoRA and why it's being used.\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models (LLMs) with minimal accuracy compromise. It aims to efficiently increase the context window of pre-trained LLMs, such as Llama2 models, by introducing shifted sparse attention (S2-Attn) during training. This method allows for extending the context length while reducing GPU memory cost and training time compared to standard full fine-tuning. LongLoRA is used to improve the performance of LLMs on tasks requiring longer context sizes, such as summarizing long documents or answering long questions.\n",
      "=== Calling Function ===\n",
      "Calling function: lora_paper_vector_query_engine_tool with args: {\"input\": \"Lora\"}\n",
      "=== Function Output ===\n",
      "LoRA is a method that allows for adaptation without requiring full-rank gradient updates to weight matrices. By setting the LoRA rank appropriately, it can approximate the expressiveness of full fine-tuning. When deployed in production, LoRA incurs no additional inference latency, as the necessary computations can be performed efficiently. It exhibits favorable sample-efficiency compared to other methods like fine-tuning, making it a promising approach for adaptation tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: longlora_efficient_fine_tuning_vector_query_engine_tool with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs) with limited computation cost. It combines shifted sparse attention (S2-Attn) for effective context extension with improved LoRA for context expansion under the premise of trainable embedding and normalization. LongLoRA demonstrates strong empirical results on various tasks and extends Llama2 models' context while retaining their original architectures. It is compatible with existing techniques like Flash-Attention2 and offers up to 1.8x lower memory cost than full fine-tuning while improving training speed.\n",
      "=== LLM Response ===\n",
      "### Lora:\n",
      "- **Definition**: Lora, or Low-Rank Adaptation, is a method used in deep learning to adapt large-scale pre-trained language models to specific tasks or domains. It involves introducing trainable rank decomposition matrices into each layer of the Transformer architecture while freezing the pre-trained model weights.\n",
      "- **Purpose**: Lora is utilized to address the challenges associated with full fine-tuning, especially for models with a large number of parameters like GPT-3. It significantly reduces the number of trainable parameters for downstream tasks, making it more memory and computationally efficient.\n",
      "\n",
      "### LongLoRA:\n",
      "- **Definition**: LongLoRA is an efficient fine-tuning approach that extends the context length of large language models (LLMs) with minimal accuracy compromise. It aims to efficiently increase the context window of pre-trained LLMs, such as Llama2 models, by introducing shifted sparse attention (S2-Attn) during training.\n",
      "- **Purpose**: LongLoRA is used to improve the performance of LLMs on tasks requiring longer context sizes, such as summarizing long documents or answering long questions. It reduces GPU memory cost and training time compared to standard full fine-tuning.\n",
      "\n",
      "### Comparison:\n",
      "- **Similarities**:\n",
      "  - Both Lora and LongLoRA aim to adapt pre-trained language models to specific tasks efficiently.\n",
      "  - They both introduce innovative techniques to reduce memory and computational costs compared to traditional full fine-tuning methods.\n",
      "\n",
      "- **Differences**:\n",
      "  - Lora focuses on adapting large-scale pre-trained models by introducing trainable rank decomposition matrices, while LongLoRA specifically extends the context length of LLMs using shifted sparse attention.\n",
      "  - LongLoRA is designed to address the challenges of longer context sizes, while Lora is more broadly applicable to adapting models to various tasks.\n",
      "  - LongLoRA offers improved memory cost and training speed benefits compared to Lora, making it more suitable for tasks requiring longer context lengths.\n",
      "\n",
      "In summary, while Lora and LongLoRA share the goal of efficient adaptation of pre-trained models, they differ in their specific techniques and target applications, with LongLoRA being more specialized for extending context lengths with improved efficiency.\n",
      "assistant: ### Lora:\n",
      "- **Definition**: Lora, or Low-Rank Adaptation, is a method used in deep learning to adapt large-scale pre-trained language models to specific tasks or domains. It involves introducing trainable rank decomposition matrices into each layer of the Transformer architecture while freezing the pre-trained model weights.\n",
      "- **Purpose**: Lora is utilized to address the challenges associated with full fine-tuning, especially for models with a large number of parameters like GPT-3. It significantly reduces the number of trainable parameters for downstream tasks, making it more memory and computationally efficient.\n",
      "\n",
      "### LongLoRA:\n",
      "- **Definition**: LongLoRA is an efficient fine-tuning approach that extends the context length of large language models (LLMs) with minimal accuracy compromise. It aims to efficiently increase the context window of pre-trained LLMs, such as Llama2 models, by introducing shifted sparse attention (S2-Attn) during training.\n",
      "- **Purpose**: LongLoRA is used to improve the performance of LLMs on tasks requiring longer context sizes, such as summarizing long documents or answering long questions. It reduces GPU memory cost and training time compared to standard full fine-tuning.\n",
      "\n",
      "### Comparison:\n",
      "- **Similarities**:\n",
      "  - Both Lora and LongLoRA aim to adapt pre-trained language models to specific tasks efficiently.\n",
      "  - They both introduce innovative techniques to reduce memory and computational costs compared to traditional full fine-tuning methods.\n",
      "\n",
      "- **Differences**:\n",
      "  - Lora focuses on adapting large-scale pre-trained models by introducing trainable rank decomposition matrices, while LongLoRA specifically extends the context length of LLMs using shifted sparse attention.\n",
      "  - LongLoRA is designed to address the challenges of longer context sizes, while Lora is more broadly applicable to adapting models to various tasks.\n",
      "  - LongLoRA offers improved memory cost and training speed benefits compared to Lora, making it more suitable for tasks requiring longer context lengths.\n",
      "\n",
      "In summary, while Lora and LongLoRA share the goal of efficient adaptation of pre-trained models, they differ in their specific techniques and target applications, with LongLoRA being more specialized for extending context lengths with improved efficiency.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Explain to me what is Lora and why it's being used.\"\n",
    "    \"Explain to me what is LongLoRA and why it's being used.\"\n",
    "    \"Compare and contract LongLoRA and Lora.\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Write me a summary of the LoRA paper.Write me a summary of the LongLoRA paper.Compare and contract LongLoRA and Lora.\n",
      "=== Calling Function ===\n",
      "Calling function: lora_paper_summary_query_engine_tool with args: {\"input\": \"summary\"}\n",
      "=== Function Output ===\n",
      "LoRA is a method proposed for efficient adaptation of large language models by injecting trainable rank decomposition matrices into each layer of the Transformer architecture. It reduces the number of trainable parameters, leading to reduced GPU memory requirements, while maintaining model quality comparable to full fine-tuning. Experiments focused on aspects like LORA module correlation, rank effects in GPT-2, amplification factor in low-rank matrices, and W and ∆W correlation, providing insights into model performance under different conditions.\n",
      "=== Calling Function ===\n",
      "Calling function: longlora_efficient_fine_tuning_summary_query_engine_tool with args: {\"input\": \"summary\"}\n",
      "=== Function Output ===\n",
      "A method called LongLoRA has been developed to extend the context length of large language models (LLMs) efficiently without compromising accuracy. It utilizes Shifted Sparse Attention (S2-Attn) during training to approximate standard self-attention patterns. LongLoRA incorporates trainable normalization and embedding layers to bridge the gap between low-rank adaptation and full fine-tuning, enabling the extension of LLMs to significantly larger context lengths. The method has shown promising results on various tasks and datasets, demonstrating effectiveness and efficiency in extending the context window of LLMs. Additionally, a framework focusing on enhancing forgery detection through the Action Units Relation Learning framework has been proposed. It includes the Action Units Relation Transformer (ART) for capturing intra-face relations and the Tampered AU Prediction (TAP) for generating challenging samples for model learning. This framework has achieved state-of-the-art performance in cross-dataset evaluations, provides visualizations of tampered regions, and improves model generalization.\n",
      "=== Calling Function ===\n",
      "Calling function: lora_paper_vector_query_engine_tool with args: {\"input\": \"LoRA\"}\n",
      "=== Function Output ===\n",
      "LoRA is a method that allows for adaptation without requiring the accumulated gradient update to weight matrices to have full-rank. By setting the LoRA rank to the rank of the pre-trained weight matrices, it can achieve a level of expressiveness similar to full fine-tuning. When deployed in production, LoRA does not introduce additional inference latency, as the inference can be performed as usual by explicitly computing and storing certain matrices. Additionally, LoRA has shown competitive performance with a small rank, suggesting that a low-rank adaptation matrix may be sufficient for effective adaptation in certain cases.\n",
      "=== Calling Function ===\n",
      "Calling function: longlora_efficient_fine_tuning_vector_query_engine_tool with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs) with limited computation cost. It combines shifted sparse attention (S2-Attn) for effective context extension with improved LoRA for context expansion. LongLoRA demonstrates strong empirical results on various tasks and extends Llama2 models' context while retaining their original architectures. It is compatible with existing techniques like Flash-Attention2 and offers significant computation savings and improved training speed compared to conventional methods.\n",
      "=== LLM Response ===\n",
      "### Summary of LoRA Paper:\n",
      "LoRA is a method proposed for efficient adaptation of large language models by injecting trainable rank decomposition matrices into each layer of the Transformer architecture. It reduces the number of trainable parameters, leading to reduced GPU memory requirements, while maintaining model quality comparable to full fine-tuning. Experiments focused on aspects like LORA module correlation, rank effects in GPT-2, amplification factor in low-rank matrices, and W and ΔW correlation, providing insights into model performance under different conditions.\n",
      "\n",
      "### Summary of LongLoRA Paper:\n",
      "A method called LongLoRA has been developed to extend the context length of large language models (LLMs) efficiently without compromising accuracy. It utilizes Shifted Sparse Attention (S2-Attn) during training to approximate standard self-attention patterns. LongLoRA incorporates trainable normalization and embedding layers to bridge the gap between low-rank adaptation and full fine-tuning, enabling the extension of LLMs to significantly larger context lengths. The method has shown promising results on various tasks and datasets, demonstrating effectiveness and efficiency in extending the context window of LLMs.\n",
      "\n",
      "### Comparison of LoRA and LongLoRA:\n",
      "- **LoRA**:\n",
      "  - Allows for adaptation without requiring full-rank gradient updates to weight matrices.\n",
      "  - Achieves expressiveness similar to full fine-tuning by setting the LoRA rank to the rank of pre-trained weight matrices.\n",
      "  - Competitive performance with a small rank, suggesting effective adaptation with low-rank matrices.\n",
      "  - Does not introduce additional inference latency in production.\n",
      "\n",
      "- **LongLoRA**:\n",
      "  - Efficiently extends the context sizes of pre-trained large language models with limited computation cost.\n",
      "  - Combines shifted sparse attention (S2-Attn) with improved LoRA for context expansion.\n",
      "  - Demonstrates strong empirical results on various tasks and extends LLMs' context while retaining their original architectures.\n",
      "  - Offers significant computation savings and improved training speed compared to conventional methods.\n",
      "assistant: ### Summary of LoRA Paper:\n",
      "LoRA is a method proposed for efficient adaptation of large language models by injecting trainable rank decomposition matrices into each layer of the Transformer architecture. It reduces the number of trainable parameters, leading to reduced GPU memory requirements, while maintaining model quality comparable to full fine-tuning. Experiments focused on aspects like LORA module correlation, rank effects in GPT-2, amplification factor in low-rank matrices, and W and ΔW correlation, providing insights into model performance under different conditions.\n",
      "\n",
      "### Summary of LongLoRA Paper:\n",
      "A method called LongLoRA has been developed to extend the context length of large language models (LLMs) efficiently without compromising accuracy. It utilizes Shifted Sparse Attention (S2-Attn) during training to approximate standard self-attention patterns. LongLoRA incorporates trainable normalization and embedding layers to bridge the gap between low-rank adaptation and full fine-tuning, enabling the extension of LLMs to significantly larger context lengths. The method has shown promising results on various tasks and datasets, demonstrating effectiveness and efficiency in extending the context window of LLMs.\n",
      "\n",
      "### Comparison of LoRA and LongLoRA:\n",
      "- **LoRA**:\n",
      "  - Allows for adaptation without requiring full-rank gradient updates to weight matrices.\n",
      "  - Achieves expressiveness similar to full fine-tuning by setting the LoRA rank to the rank of pre-trained weight matrices.\n",
      "  - Competitive performance with a small rank, suggesting effective adaptation with low-rank matrices.\n",
      "  - Does not introduce additional inference latency in production.\n",
      "\n",
      "- **LongLoRA**:\n",
      "  - Efficiently extends the context sizes of pre-trained large language models with limited computation cost.\n",
      "  - Combines shifted sparse attention (S2-Attn) with improved LoRA for context expansion.\n",
      "  - Demonstrates strong empirical results on various tasks and extends LLMs' context while retaining their original architectures.\n",
      "  - Offers significant computation savings and improved training speed compared to conventional methods.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Write me a summary of the LoRA paper.\"\n",
    "    \"Write me a summary of the LongLoRA paper.\"\n",
    "    \"Compare and contract LongLoRA and Lora.\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Documents Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://arxiv.org/pdf/2106.09685\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"lora_paper.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-11 16:08:51--  https://arxiv.org/pdf/2106.09685\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.3.42, 151.101.131.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1609513 (1.5M) [application/pdf]\n",
      "Saving to: ‘lora_paper.pdf’\n",
      "\n",
      "lora_paper.pdf      100%[===================>]   1.53M  1.63MB/s    in 0.9s    \n",
      "\n",
      "2024-05-11 16:08:53 (1.63 MB/s) - ‘lora_paper.pdf’ saved [1609513/1609513]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# poetry add wget\n",
    "\n",
    "import wget\n",
    "\n",
    "for url, paper in zip(urls, papers):\n",
    "    !wget \"{url}\" -O \"{paper}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [\n",
    "    \"./datasets/lora_paper.pdf\",\n",
    "    \"./datasets/longlora_efficient_fine_tuning.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ./datasets/lora_paper.pdf paper tool.\n",
      "Creating ./datasets/longlora_efficient_fine_tuning.pdf paper tool.\n"
     ]
    }
   ],
   "source": [
    "from utils import create_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "\n",
    "for paper in papers:\n",
    "    print(f\"Creating {paper} paper tool.\")\n",
    "    path = Path(paper)\n",
    "    vector_tool, summary_tool = await create_doc_tools(doc_name=path.stem, document_fp=path)\n",
    "    paper_to_tools_dict[path.stem] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.core.tools.query_engine.QueryEngineTool object at 0x7864e7627850>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7864e76254e0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7864e7746290>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7864e77476a0>]\n"
     ]
    }
   ],
   "source": [
    "tools_list = [t for paper in papers for t in paper_to_tools_dict[Path(paper).stem]]\n",
    "print(str(tools_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    tools_list,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.core.tools.query_engine.QueryEngineTool object at 0x7864e76254e0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7864e77476a0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7864e7627850>]\n"
     ]
    }
   ],
   "source": [
    "retrieved_tools = obj_retriever.retrieve(\n",
    "    \"Write me a summary of the LoRA paper.\"\n",
    "    \"Write me a summary of the LongLoRA paper.\"\n",
    "    \"Compare and contract LongLoRA and Lora.\"\n",
    ")\n",
    "print(str(retrieved_tools))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_paper_summary_query_engine_tool\n",
      "longlora_efficient_fine_tuning_summary_query_engine_tool\n",
      "lora_paper_vector_query_engine_tool\n"
     ]
    }
   ],
   "source": [
    "for tool in retrieved_tools:\n",
    "    print(tool.metadata.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an AI agent programmed to respond to questions based on a \n",
    "specified collection of documents. Always utilize the tools available \n",
    "to generate answers, ensuring that responses are based directly on the \n",
    "provided materials rather than on any pre-existing knowledge. All your responses should be formatted in markdown text\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Write me a summary of the LoRA paper.Write me a summary of the LongLoRA paper.Compare and contract LongLoRA and Lora.\n",
      "=== Calling Function ===\n",
      "Calling function: lora_paper_summary_query_engine_tool with args: {\"input\": \"summary\"}\n",
      "=== Function Output ===\n",
      "LoRA is an innovative adaptation strategy for large language models that involves freezing pre-trained model weights and incorporating trainable rank decomposition matrices into each layer of the Transformer architecture. This method has been proven to enhance model quality compared to traditional fine-tuning approaches, while also reducing GPU memory requirements and training throughput. It enables swift task-switching during deployment without introducing additional inference latency. Empirical validation on various language models like RoBERTa, DeBERTa, GPT-2, and GPT-3 has demonstrated the effectiveness of LoRA in maintaining high performance with fewer trainable parameters. The experiments conducted in the research paper explored different aspects of LoRA, including the correlation between LoRA modules, the impact of various ranks in GPT-2, the amplification factor in low-rank matrices, and the relationship between W and ∆W to gain insights into the behavior and performance of the LoRA adaptation method in diverse scenarios and settings.\n",
      "=== Calling Function ===\n",
      "Calling function: longlora_efficient_fine_tuning_summary_query_engine_tool with args: {\"input\": \"summary\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends context sizes of pre-trained large language models with minimal accuracy compromise, utilizing Shifted Sparse Attention during training. It allows for extended context lengths efficiently, up to 100k for 7B models and 32k for 70B models on a single 8×A100 machine. LongLoRA has shown success in tasks like long-sequence language modeling and passkey retrieval accuracy. On the other hand, the proposed framework introduces the Action Units Relation Learning framework, including the Action Units Relation Transformer (ART) and Tampered AU Prediction (TAP), enhancing forgery detection and model generalization in detecting deepfakes.\n",
      "=== Calling Function ===\n",
      "Calling function: lora_paper_vector_query_engine_tool with args: {\"input\": \"LoRA\"}\n",
      "=== Function Output ===\n",
      "LoRA is a method that allows for adaptive learning without requiring full-rank gradient updates to weight matrices during adaptation. By setting the LoRA rank to the rank of the pre-trained weight matrices, it can achieve a level of expressiveness similar to full fine-tuning. When deployed in production, LoRA incurs no additional inference latency as it can efficiently compute and store the necessary matrices for inference. The method has shown competitive performance with a small rank, indicating that a low-rank adaptation matrix may be sufficient for effective adaptation in certain scenarios.\n",
      "=== Calling Function ===\n",
      "Calling function: lora_paper_vector_query_engine_tool with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA refers to the process of training LoRA with a high rank parameter, specifically setting the LoRA rank r to be equal to the dimensionality of the pre-trained weight matrices. This approach aims to retain the expressive capabilities of full fine-tuning, essentially converging the training of LoRA to that of the original model.\n",
      "=== LLM Response ===\n",
      "### Summary of LoRA Paper:\n",
      "LoRA is an innovative adaptation strategy for large language models that involves freezing pre-trained model weights and incorporating trainable rank decomposition matrices into each layer of the Transformer architecture. This method enhances model quality compared to traditional fine-tuning approaches, reduces GPU memory requirements, and enables swift task-switching during deployment without introducing additional inference latency. Empirical validation on various language models has demonstrated the effectiveness of LoRA in maintaining high performance with fewer trainable parameters.\n",
      "\n",
      "### Summary of LongLoRA Paper:\n",
      "LongLoRA is an efficient fine-tuning approach that extends context sizes of pre-trained large language models with minimal accuracy compromise, utilizing Shifted Sparse Attention during training. It allows for extended context lengths efficiently and has shown success in tasks like long-sequence language modeling and passkey retrieval accuracy. The proposed framework includes the Action Units Relation Learning framework, enhancing forgery detection and model generalization in detecting deepfakes.\n",
      "\n",
      "### Comparison and Contrast of LongLoRA and LoRA:\n",
      "- **LoRA**:\n",
      "  - Utilizes trainable rank decomposition matrices.\n",
      "  - Enhances model quality and reduces GPU memory requirements.\n",
      "  - Enables swift task-switching during deployment without latency.\n",
      "  - Empirical validation on various language models.\n",
      "  \n",
      "- **LongLoRA**:\n",
      "  - Extends context sizes of pre-trained models efficiently.\n",
      "  - Utilizes Shifted Sparse Attention during training.\n",
      "  - Success in long-sequence language modeling and passkey retrieval.\n",
      "  - Includes the Action Units Relation Learning framework for forgery detection and model generalization.\n",
      "\n",
      "Both methods focus on enhancing large language models but differ in their specific approaches and applications, with LoRA emphasizing adaptive learning and reduced memory requirements, while LongLoRA focuses on extending context sizes and improving model generalization for specific tasks like forgery detection.\n",
      "assistant: ### Summary of LoRA Paper:\n",
      "LoRA is an innovative adaptation strategy for large language models that involves freezing pre-trained model weights and incorporating trainable rank decomposition matrices into each layer of the Transformer architecture. This method enhances model quality compared to traditional fine-tuning approaches, reduces GPU memory requirements, and enables swift task-switching during deployment without introducing additional inference latency. Empirical validation on various language models has demonstrated the effectiveness of LoRA in maintaining high performance with fewer trainable parameters.\n",
      "\n",
      "### Summary of LongLoRA Paper:\n",
      "LongLoRA is an efficient fine-tuning approach that extends context sizes of pre-trained large language models with minimal accuracy compromise, utilizing Shifted Sparse Attention during training. It allows for extended context lengths efficiently and has shown success in tasks like long-sequence language modeling and passkey retrieval accuracy. The proposed framework includes the Action Units Relation Learning framework, enhancing forgery detection and model generalization in detecting deepfakes.\n",
      "\n",
      "### Comparison and Contrast of LongLoRA and LoRA:\n",
      "- **LoRA**:\n",
      "  - Utilizes trainable rank decomposition matrices.\n",
      "  - Enhances model quality and reduces GPU memory requirements.\n",
      "  - Enables swift task-switching during deployment without latency.\n",
      "  - Empirical validation on various language models.\n",
      "  \n",
      "- **LongLoRA**:\n",
      "  - Extends context sizes of pre-trained models efficiently.\n",
      "  - Utilizes Shifted Sparse Attention during training.\n",
      "  - Success in long-sequence language modeling and passkey retrieval.\n",
      "  - Includes the Action Units Relation Learning framework for forgery detection and model generalization.\n",
      "\n",
      "Both methods focus on enhancing large language models but differ in their specific approaches and applications, with LoRA emphasizing adaptive learning and reduced memory requirements, while LongLoRA focuses on extending context sizes and improving model generalization for specific tasks like forgery detection.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Write me a summary of the LoRA paper.\"\n",
    "    \"Write me a summary of the LongLoRA paper.\"\n",
    "    \"Compare and contract LongLoRA and Lora.\"\n",
    ")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
