{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load lora_paper.pdf documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"./datasets/lora_paper.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# chunk_size of 1024 is a good default value\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# Create nodes from documents\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# LLM model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "# embedding model\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "# summary index\n",
    "summary_index = SummaryIndex(nodes)\n",
    "# vector store index\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary query engine\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "# vector query engine\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to the Lora paper.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the the Lora paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Agent Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain to me what Lora and why it's being used. Are existing solutions not good enough?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Explain what Lora is and why it is being used.\"}\n",
      "=== Function Output ===\n",
      "Lora is a method used to adapt large-scale pre-trained language models to specific tasks or domains by introducing trainable rank decomposition matrices into each layer of the Transformer architecture. It aims to improve performance on target tasks by updating the pre-trained model weights efficiently without the need for extensive retraining from scratch. Lora significantly reduces the number of trainable parameters, making it more memory and computationally efficient for downstream tasks. It allows for switching tasks efficiently by replacing the rank decomposition matrices, reducing storage requirements and task-switching overhead. Additionally, Lora enables training with fewer GPUs, avoids I/O bottlenecks, and provides a speedup during training compared to full fine-tuning.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Are existing solutions not good enough for the use of Lora?\"}\n",
      "=== Function Output ===\n",
      "Existing solutions are not deemed sufficient for the use of LoRA, as they often introduce inference latency and may not match the fine-tuning baselines in terms of efficiency and model quality. LoRA, on the other hand, offers a more efficient adaptation strategy without introducing inference latency, allowing for quick task-switching and retaining high model quality.\n",
      "=== LLM Response ===\n",
      "Lora is a method used to adapt large-scale pre-trained language models to specific tasks or domains by introducing trainable rank decomposition matrices into each layer of the Transformer architecture. It aims to improve performance on target tasks by updating the pre-trained model weights efficiently without the need for extensive retraining from scratch. Lora significantly reduces the number of trainable parameters, making it more memory and computationally efficient for downstream tasks. It allows for switching tasks efficiently by replacing the rank decomposition matrices, reducing storage requirements and task-switching overhead. Additionally, Lora enables training with fewer GPUs, avoids I/O bottlenecks, and provides a speedup during training compared to full fine-tuning.\n",
      "\n",
      "Existing solutions are not deemed sufficient for the use of Lora, as they often introduce inference latency and may not match the fine-tuning baselines in terms of efficiency and model quality. Lora, on the other hand, offers a more efficient adaptation strategy without introducing inference latency, allowing for quick task-switching and retaining high model quality.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Explain to me what Lora and why it's being used. Are existing solutions not good enough?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: lora_paper.pdf\n",
      "file_path: datasets/lora_paper.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 1609513\n",
      "creation_date: 2024-05-10\n",
      "last_modified_date: 2024-05-10\n",
      "\n",
      "LORA: L OW-RANK ADAPTATION OF LARGE LAN-\n",
      "GUAGE MODELS\n",
      "Edward Hu‚àóYelong Shen‚àóPhillip Wallis Zeyuan Allen-Zhu\n",
      "Yuanzhi Li Shean Wang Lu Wang Weizhu Chen\n",
      "Microsoft Corporation\n",
      "{edwardhu, yeshe, phwallis, zeyuana,\n",
      "yuanzhil, swang, luw, wzchen }@microsoft.com\n",
      "yuanzhil@andrew.cmu.edu\n",
      "(Version 2)\n",
      "ABSTRACT\n",
      "An important paradigm of natural language processing consists of large-scale pre-\n",
      "training on general domain data and adaptation to particular tasks or domains. As\n",
      "we pre-train larger models, full Ô¨Åne-tuning, which retrains all model parameters,\n",
      "becomes less feasible. Using GPT-3 175B as an example ‚Äì deploying indepen-\n",
      "dent instances of Ô¨Åne-tuned models, each with 175B parameters, is prohibitively\n",
      "expensive. We propose Low-RankAdaptation, or LoRA, which freezes the pre-\n",
      "trained model weights and injects trainable rank decomposition matrices into each\n",
      "layer of the Transformer architecture, greatly reducing the number of trainable pa-\n",
      "rameters for downstream tasks. Compared to GPT-3 175B Ô¨Åne-tuned with Adam,\n",
      "LoRA can reduce the number of trainable parameters by 10,000 times and the\n",
      "GPU memory requirement by 3 times. LoRA performs on-par or better than Ô¨Åne-\n",
      "tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\n",
      "ing fewer trainable parameters, a higher training throughput, and, unlike adapters,\n",
      "no additional inference latency . We also provide an empirical investigation into\n",
      "rank-deÔ¨Åciency in language model adaptation, which sheds light on the efÔ¨Åcacy of\n",
      "LoRA. We release a package that facilitates the integration of LoRA with PyTorch\n",
      "models and provide our implementations and model checkpoints for RoBERTa,\n",
      "DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .\n",
      "1 I NTRODUCTION\n",
      "Pretrained \n",
      "Weights\n",
      "ùëä‚àà‚Ñùùëë√óùëë\n",
      "xh\n",
      "ùêµ=0\n",
      "ùê¥=ùí©(0,ùúé2)\n",
      "ùëëùëüPretrained \n",
      "Weights\n",
      "ùëä‚àà‚Ñùùëë√óùëë\n",
      "xf(x)\n",
      "ùëë\n",
      "Figure 1: Our reparametriza-\n",
      "tion. We only train AandB.Many applications in natural language processing rely on adapt-\n",
      "ingonelarge-scale, pre-trained language model to multiple down-\n",
      "stream applications. Such adaptation is usually done via Ô¨Åne-tuning ,\n",
      "which updates all the parameters of the pre-trained model. The ma-\n",
      "jor downside of Ô¨Åne-tuning is that the new model contains as many\n",
      "parameters as in the original model. As larger models are trained\n",
      "every few months, this changes from a mere ‚Äúinconvenience‚Äù for\n",
      "GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\n",
      "critical deployment challenge for GPT-3 (Brown et al., 2020) with\n",
      "175 billion trainable parameters.1\n",
      "Many sought to mitigate this by adapting only some parameters or\n",
      "learning external modules for new tasks. This way, we only need\n",
      "to store and load a small number of task-speciÔ¨Åc parameters in ad-\n",
      "dition to the pre-trained model for each task, greatly boosting the\n",
      "operational efÔ¨Åciency when deployed. However, existing techniques\n",
      "‚àóEqual contribution.\n",
      "0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\n",
      "1While GPT-3 175B achieves non-trivial performance with few-shot learning, Ô¨Åne-tuning boosts its perfor-\n",
      "mance signiÔ¨Åcantly as shown in Appendix A.\n",
      "1arXiv:2106.09685v2  [cs.CL]  16 Oct 2021\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
