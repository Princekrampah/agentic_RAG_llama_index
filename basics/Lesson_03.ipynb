{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load lora_paper.pdf documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"./datasets/lora_paper.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# chunk_size of 1024 is a good default value\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# Create nodes from documents\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# LLM model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "# embedding model\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "# summary index\n",
    "summary_index = SummaryIndex(nodes)\n",
    "# vector store index\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary query engine\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "# vector query engine\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to the Lora paper.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the the Lora paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Agent Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain to me what Lora and why it's being used. Are existing solutions not good enough?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Explain what Lora is and why it is being used.\"}\n",
      "=== Function Output ===\n",
      "Lora is a method used to adapt large-scale pre-trained language models to specific tasks or domains by introducing trainable rank decomposition matrices into each layer of the Transformer architecture. It aims to improve performance on target tasks by updating the pre-trained model weights efficiently without the need for extensive retraining from scratch. Lora significantly reduces the number of trainable parameters, making it more memory and computationally efficient for downstream tasks. It allows for switching tasks efficiently by replacing the rank decomposition matrices, reducing storage requirements and task-switching overhead. Additionally, Lora enables training with fewer GPUs, avoids I/O bottlenecks, and provides a speedup during training compared to full fine-tuning.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Are existing solutions not good enough for the use of Lora?\"}\n",
      "=== Function Output ===\n",
      "Existing solutions are not deemed sufficient for the use of LoRA, as they often introduce inference latency and may not match the fine-tuning baselines in terms of efficiency and model quality. LoRA, on the other hand, offers a more efficient adaptation strategy without introducing inference latency, allowing for quick task-switching and retaining high model quality.\n",
      "=== LLM Response ===\n",
      "Lora is a method used to adapt large-scale pre-trained language models to specific tasks or domains by introducing trainable rank decomposition matrices into each layer of the Transformer architecture. It aims to improve performance on target tasks by updating the pre-trained model weights efficiently without the need for extensive retraining from scratch. Lora significantly reduces the number of trainable parameters, making it more memory and computationally efficient for downstream tasks. It allows for switching tasks efficiently by replacing the rank decomposition matrices, reducing storage requirements and task-switching overhead. Additionally, Lora enables training with fewer GPUs, avoids I/O bottlenecks, and provides a speedup during training compared to full fine-tuning.\n",
      "\n",
      "Existing solutions are not deemed sufficient for the use of Lora, as they often introduce inference latency and may not match the fine-tuning baselines in terms of efficiency and model quality. Lora, on the other hand, offers a more efficient adaptation strategy without introducing inference latency, allowing for quick task-switching and retaining high model quality.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Explain to me what is Lora and why it's being used. Are existing solutions not good enough?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: lora_paper.pdf\n",
      "file_path: datasets/lora_paper.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 1609513\n",
      "creation_date: 2024-05-10\n",
      "last_modified_date: 2024-05-10\n",
      "\n",
      "LORA: L OW-RANK ADAPTATION OF LARGE LAN-\n",
      "GUAGE MODELS\n",
      "Edward Hu‚àóYelong Shen‚àóPhillip Wallis Zeyuan Allen-Zhu\n",
      "Yuanzhi Li Shean Wang Lu Wang Weizhu Chen\n",
      "Microsoft Corporation\n",
      "{edwardhu, yeshe, phwallis, zeyuana,\n",
      "yuanzhil, swang, luw, wzchen }@microsoft.com\n",
      "yuanzhil@andrew.cmu.edu\n",
      "(Version 2)\n",
      "ABSTRACT\n",
      "An important paradigm of natural language processing consists of large-scale pre-\n",
      "training on general domain data and adaptation to particular tasks or domains. As\n",
      "we pre-train larger models, full Ô¨Åne-tuning, which retrains all model parameters,\n",
      "becomes less feasible. Using GPT-3 175B as an example ‚Äì deploying indepen-\n",
      "dent instances of Ô¨Åne-tuned models, each with 175B parameters, is prohibitively\n",
      "expensive. We propose Low-RankAdaptation, or LoRA, which freezes the pre-\n",
      "trained model weights and injects trainable rank decomposition matrices into each\n",
      "layer of the Transformer architecture, greatly reducing the number of trainable pa-\n",
      "rameters for downstream tasks. Compared to GPT-3 175B Ô¨Åne-tuned with Adam,\n",
      "LoRA can reduce the number of trainable parameters by 10,000 times and the\n",
      "GPU memory requirement by 3 times. LoRA performs on-par or better than Ô¨Åne-\n",
      "tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\n",
      "ing fewer trainable parameters, a higher training throughput, and, unlike adapters,\n",
      "no additional inference latency . We also provide an empirical investigation into\n",
      "rank-deÔ¨Åciency in language model adaptation, which sheds light on the efÔ¨Åcacy of\n",
      "LoRA. We release a package that facilitates the integration of LoRA with PyTorch\n",
      "models and provide our implementations and model checkpoints for RoBERTa,\n",
      "DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .\n",
      "1 I NTRODUCTION\n",
      "Pretrained \n",
      "Weights\n",
      "ùëä‚àà‚Ñùùëë√óùëë\n",
      "xh\n",
      "ùêµ=0\n",
      "ùê¥=ùí©(0,ùúé2)\n",
      "ùëëùëüPretrained \n",
      "Weights\n",
      "ùëä‚àà‚Ñùùëë√óùëë\n",
      "xf(x)\n",
      "ùëë\n",
      "Figure 1: Our reparametriza-\n",
      "tion. We only train AandB.Many applications in natural language processing rely on adapt-\n",
      "ingonelarge-scale, pre-trained language model to multiple down-\n",
      "stream applications. Such adaptation is usually done via Ô¨Åne-tuning ,\n",
      "which updates all the parameters of the pre-trained model. The ma-\n",
      "jor downside of Ô¨Åne-tuning is that the new model contains as many\n",
      "parameters as in the original model. As larger models are trained\n",
      "every few months, this changes from a mere ‚Äúinconvenience‚Äù for\n",
      "GPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\n",
      "critical deployment challenge for GPT-3 (Brown et al., 2020) with\n",
      "175 billion trainable parameters.1\n",
      "Many sought to mitigate this by adapting only some parameters or\n",
      "learning external modules for new tasks. This way, we only need\n",
      "to store and load a small number of task-speciÔ¨Åc parameters in ad-\n",
      "dition to the pre-trained model for each task, greatly boosting the\n",
      "operational efÔ¨Åciency when deployed. However, existing techniques\n",
      "‚àóEqual contribution.\n",
      "0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\n",
      "1While GPT-3 175B achieves non-trivial performance with few-shot learning, Ô¨Åne-tuning boosts its perfor-\n",
      "mance signiÔ¨Åcantly as shown in Appendix A.\n",
      "1arXiv:2106.09685v2  [cs.CL]  16 Oct 2021\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Chat Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain to me what is Lora and why it's being used. Are existing solutions not good enough?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Explain what Lora is and why it is being used.\"}\n",
      "=== Function Output ===\n",
      "Lora is a method used in deep learning for adapting large-scale pre-trained language models to specific tasks or domains. It involves freezing the pre-trained model weights and introducing trainable rank decomposition matrices into each layer of the Transformer architecture. This approach significantly reduces the number of trainable parameters for downstream tasks, making training more efficient and lowering hardware requirements. Lora is employed to fine-tune models for improved performance on specific tasks without the need to retrain the entire model from scratch, thus optimizing computational resources while enhancing task-specific capabilities.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Are existing solutions not good enough for the use of Lora?\"}\n",
      "=== Function Output ===\n",
      "Existing solutions are not deemed sufficient for the use of LoRA, as they introduce inference latency and may not match the performance of full fine-tuning. LoRA, on the other hand, offers an efficient adaptation strategy without introducing latency and maintains high model quality, making it a more favorable option compared to existing methods.\n",
      "=== LLM Response ===\n",
      "Lora is a method used in deep learning for adapting large-scale pre-trained language models to specific tasks or domains. It involves freezing the pre-trained model weights and introducing trainable rank decomposition matrices into each layer of the Transformer architecture. This approach significantly reduces the number of trainable parameters for downstream tasks, making training more efficient and lowering hardware requirements. Lora is employed to fine-tune models for improved performance on specific tasks without the need to retrain the entire model from scratch, thus optimizing computational resources while enhancing task-specific capabilities.\n",
      "\n",
      "Existing solutions are not deemed sufficient for the use of Lora, as they introduce inference latency and may not match the performance of full fine-tuning. Lora, on the other hand, offers an efficient adaptation strategy without introducing latency and maintains high model quality, making it a more favorable option compared to existing methods.\n",
      "assistant: Lora is a method used in deep learning for adapting large-scale pre-trained language models to specific tasks or domains. It involves freezing the pre-trained model weights and introducing trainable rank decomposition matrices into each layer of the Transformer architecture. This approach significantly reduces the number of trainable parameters for downstream tasks, making training more efficient and lowering hardware requirements. Lora is employed to fine-tune models for improved performance on specific tasks without the need to retrain the entire model from scratch, thus optimizing computational resources while enhancing task-specific capabilities.\n",
      "\n",
      "Existing solutions are not deemed sufficient for the use of Lora, as they introduce inference latency and may not match the performance of full fine-tuning. Lora, on the other hand, offers an efficient adaptation strategy without introducing latency and maintains high model quality, making it a more favorable option compared to existing methods.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Explain to me what is Lora and why it's being used. Are existing solutions not good enough?\"\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What was my last question to you?\n",
      "=== LLM Response ===\n",
      "Your last question to me was: \"Explain to me what is Lora and why it's being used. Are existing solutions not good enough?\"\n",
      "assistant: Your last question to me was: \"Explain to me what is Lora and why it's being used. Are existing solutions not good enough?\"\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"What was my last question to you?\"\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low-Level Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool], \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating A Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "    \"Explain to me what is Lora and why it's being used.\"\n",
    "    \"Are existing solutions not good enough?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of completed steps for tasksID 21671153-bb37-4ef2-a653-bd9090c39d00 is 0\n"
     ]
    }
   ],
   "source": [
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "\n",
    "print(f\"Number of completed steps for tasksID {task.task_id} is {len(completed_steps)}\")\n",
    "\n",
    "if len(completed_steps) > 0:\n",
    "    print(completed_steps[0].output.sources[0].raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of completed steps for tasksID 21671153-bb37-4ef2-a653-bd9090c39d00 is 1\n",
      "Explain to me what is Lora and why it's being used.Are existing solutions not good enough?\n"
     ]
    }
   ],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "print(f\"Number of completed steps for tasksID {task.task_id} is {len(upcoming_steps)}\")\n",
    "\n",
    "if len(upcoming_steps) > 0:\n",
    "    print(upcoming_steps[0].input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute A Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain to me what is Lora and why it's being used.Are existing solutions not good enough?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Explain what Lora is and why it is being used.\"}\n",
      "=== Function Output ===\n",
      "Lora is a method used to adapt large-scale pre-trained language models to specific tasks or domains by introducing trainable rank decomposition matrices into each layer of the Transformer architecture. It aims to reduce the number of trainable parameters for downstream tasks, making training more efficient and lowering hardware requirements. Lora allows for quick task-switching without introducing additional inference latency by updating only the injected low-rank matrices instead of all model parameters. It is being used to address the challenge of fine-tuning large models like GPT-3, which can be costly to deploy independently fine-tuned models for each task. Lora enhances model performance by amplifying task-specific directions in the model's feature space, improving its ability to handle specific tasks effectively and efficiently.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Are existing solutions not good enough for Lora technology?\"}\n",
      "=== Function Output ===\n",
      "Existing solutions may not be good enough for LoRA technology, as indicated by the need for advancements like LoRA to address specific challenges and improve performance in tasks such as model adaptation. The results from various experiments suggest that LoRA outperforms or performs comparably to other methods, showcasing the potential limitations of existing solutions in the context of LoRA technology.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of completed steps for tasksID 21671153-bb37-4ef2-a653-bd9090c39d00 is 1\n",
      "Lora is a method used to adapt large-scale pre-trained language models to specific tasks or domains by introducing trainable rank decomposition matrices into each layer of the Transformer architecture. It aims to reduce the number of trainable parameters for downstream tasks, making training more efficient and lowering hardware requirements. Lora allows for quick task-switching without introducing additional inference latency by updating only the injected low-rank matrices instead of all model parameters. It is being used to address the challenge of fine-tuning large models like GPT-3, which can be costly to deploy independently fine-tuned models for each task. Lora enhances model performance by amplifying task-specific directions in the model's feature space, improving its ability to handle specific tasks effectively and efficiently.\n"
     ]
    }
   ],
   "source": [
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "\n",
    "print(f\"Number of completed steps for tasksID {task.task_id} is {len(completed_steps)}\")\n",
    "\n",
    "if len(completed_steps) > 0:\n",
    "    print(completed_steps[0].output.sources[0].raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of completed steps for tasksID 21671153-bb37-4ef2-a653-bd9090c39d00 is 1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "print(f\"Number of completed steps for tasksID {task.task_id} is {len(upcoming_steps)}\")\n",
    "\n",
    "if len(upcoming_steps) > 0:\n",
    "    print(upcoming_steps[0].input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "Lora is a method used to adapt large-scale pre-trained language models to specific tasks or domains by introducing trainable rank decomposition matrices into each layer of the Transformer architecture. It aims to reduce the number of trainable parameters for downstream tasks, making training more efficient and lowering hardware requirements. Lora allows for quick task-switching without introducing additional inference latency by updating only the injected low-rank matrices instead of all model parameters. It is being used to address the challenge of fine-tuning large models like GPT-3, which can be costly to deploy independently fine-tuned models for each task. Lora enhances model performance by amplifying task-specific directions in the model's feature space, improving its ability to handle specific tasks effectively and efficiently.\n",
      "\n",
      "Existing solutions may not be good enough for Lora technology, as indicated by the need for advancements like Lora to address specific challenges and improve performance in tasks such as model adaptation. The results from various experiments suggest that Lora outperforms or performs comparably to other methods, showcasing the potential limitations of existing solutions in the context of Lora technology.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Human Feedback In The Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "    \"Explain to me what is Lora and why it's being used.\"\n",
    "    \"Are existing solutions not good enough?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain to me the dataset used to fine-tune in the Lora paper.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Dataset used to fine-tune in the Lora paper\"}\n",
      "=== Function Output ===\n",
      "The datasets used for fine-tuning in the LoRA paper are GLUE benchmark, WikiSQL, MultiNLI, SAMSum, E2E NLG Challenge, and GPT-3 175B.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(\n",
    "    task.task_id, input=\"Explain to me the dataset used to fine-tune in the Lora paper.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "The dataset used for fine-tuning in the Lora paper includes the GLUE benchmark, WikiSQL, MultiNLI, SAMSum, E2E NLG Challenge, and GPT-3 175B.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)\n",
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The dataset used for fine-tuning in the Lora paper includes the GLUE benchmark, WikiSQL, MultiNLI, SAMSum, E2E NLG Challenge, and GPT-3 175B.\n"
     ]
    }
   ],
   "source": [
    "response = agent.finalize_response(task.task_id)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
