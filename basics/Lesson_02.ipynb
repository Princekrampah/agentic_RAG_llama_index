{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling With Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Function Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "# substraction function\n",
    "def sub(x: int, y: int) -> int:\n",
    "    \"\"\"Substract two numbers.\"\"\"\n",
    "    return x - y\n",
    "\n",
    "# multiplication function\n",
    "def mul(x: int, y: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "# get user information\n",
    "def get_user_info(name: str) -> str:\n",
    "    \"\"\"Get user information.\"\"\"\n",
    "    data = {\n",
    "        \"John Doe\": {\n",
    "            \"age\": 30,\n",
    "            \"location\": \"USA\"\n",
    "        },\n",
    "        \"Jane Doe\": {\n",
    "            \"age\": 25,\n",
    "            \"location\": \"UK\"\n",
    "        }\n",
    "    }\n",
    "    return f'User name {name}, age is {data[name][\"age\"]} and location is {data[name][\"location\"]}'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Tool From Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "addition_tool = FunctionTool.from_defaults(fn=add)\n",
    "get_user_info_tool = FunctionTool.from_defaults(fn=get_user_info)\n",
    "multiplication_tool = FunctionTool.from_defaults(fn=mul)\n",
    "substraction_tool = FunctionTool.from_defaults(fn=sub)\n",
    "\n",
    "tools = [addition_tool, get_user_info_tool, multiplication_tool, substraction_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mul with args: {\"x\": 4, \"y\": 5}\n",
      "=== Function Output ===\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = llm.predict_and_call(\n",
    "    tools, \n",
    "    \"What is the product of 4 and 5\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: get_user_info with args: {\"name\": \"John Doe\"}\n",
      "=== Function Output ===\n",
      "User name John Doe, age is 30 and location is USA\n",
      "User name John Doe, age is 30 and location is USA\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    tools, \n",
    "    \"Give more the details of John Doe\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Vector Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load lora_paper.pdf documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"./datasets/lora_paper.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# chunk_size of 1024 is a good default value\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# Create nodes from documents\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# LLM model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "# embedding model\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "# vector store index\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Query Engine With MetadataFilters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The problem statement focuses on language modeling, particularly in the context of adapting a pre-trained autoregressive language model to downstream conditional text generation tasks. It discusses the use of a pre-trained model like GPT based on the Transformer architecture for tasks such as summarization, machine reading comprehension, and natural language to SQL. The goal is to maximize conditional probabilities given a task-specific prompt by adapting the pre-trained model to new tasks represented by context-target pairs in training datasets. Each downstream task involves sequences of tokens where the model needs to generate appropriate outputs based on the input context.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "# Create vector search query engine\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"2\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"Tell me about the Problem statement as explained\", \n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying page number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\n",
      "depth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\n",
      "bardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\n",
      "match the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality.\n",
      "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\n",
      "over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\n",
      "change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\n",
      "Low-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\n",
      "network indirectly by optimizing rank decomposition matrices of the dense layers’ change during\n",
      "adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n",
      "175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufﬁces even\n",
      "when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.\n",
      "LoRA possesses several key advantages.\n",
      "• A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
      "ferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\n",
      "matricesAandBin Figure 1, reducing the storage requirement and task-switching over-\n",
      "head signiﬁcantly.\n",
      "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\n",
      "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
      "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
      "much smaller low-rank matrices.\n",
      "• Our simple linear design allows us to merge the trainable matrices with the frozen weights\n",
      "when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\n",
      "construction.\n",
      "• LoRA is orthogonal to many prior methods and can be combined with many of them, such\n",
      "as preﬁx-tuning. We provide an example in Appendix E.\n",
      "Terminologies and Conventions We make frequent references to the Transformer architecture\n",
      "and use the conventional terminologies for its dimensions. We call the input and output di-\n",
      "mension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\n",
      "query/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\n",
      "trained weight matrix and ∆Wits accumulated gradient update during adaptation. We use rto\n",
      "denote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\n",
      "Brown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\n",
      "optimization and use a Transformer MLP feedforward dimension dffn= 4×dmodel .\n",
      "2 P ROBLEM STATEMENT\n",
      "While our proposal is agnostic to training objective, we focus on language modeling as our motivat-\n",
      "ing use case. Below is a brief description of the language modeling problem and, in particular, the\n",
      "maximization of conditional probabilities given a task-speciﬁc prompt.\n",
      "Suppose we are given a pre-trained autoregressive language model PΦ(y|x)parametrized by Φ.\n",
      "For instance, PΦ(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\n",
      "et al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\n",
      "pre-trained model to downstream conditional text generation tasks, such as summarization, machine\n",
      "reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\n",
      "represented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\n",
      "yiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\n",
      "corresponding SQL command; for summarization, xiis the content of an article and yiits summary.\n",
      "2\n",
      "=============Text=============\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"=============Text=============\")\n",
    "    print(n.get_text())\n",
    "    print(\"=============Text=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "\n",
    "def vector_search_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Conduct a vector search across an index using the following parameters:\n",
    "\n",
    "    query (str): This is the text string you want to embed and search for within the index.\n",
    "    page_numbers (List[str]): This parameter allows you to limit the search to \n",
    "    specific pages. If left empty, the search will encompass all pages in the index. \n",
    "    If page numbers are specified, the search will be filtered to only include those pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_search_tool\",\n",
    "    fn=vector_search_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_search_tool with args: {\"query\": \"problem statement\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "The problem statement focuses on language modeling, particularly on maximizing conditional probabilities given a task-specific prompt. It discusses adapting a pre-trained autoregressive language model to downstream conditional text generation tasks like summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each task is defined by a dataset of context-target pairs, where both the context and target are sequences of tokens. For instance, in NL2SQL, the context is a natural language query and the target is the corresponding SQL command; in summarization, the context is an article's content and the target is its summary.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_search_query_tool], \n",
    "    \"What was mentioned about the problem statement in page 2?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The problem statement focuses on language modeling, particularly on maximizing conditional probabilities given a task-specific prompt. It discusses adapting a pre-trained autoregressive language model to downstream conditional text generation tasks like summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each task is defined by a dataset of context-target pairs, where both the context and target are sequences of tokens. For instance, in NL2SQL, the context is a natural language query and the target is the corresponding SQL command; in summarization, the context is an article's content and the target is its summary.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_search_tool with args: {\"query\": \"problem statement\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "The problem statement focuses on language modeling, particularly on maximizing conditional probabilities given a task-specific prompt. It involves adapting a pre-trained autoregressive language model to downstream conditional text generation tasks like summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each task is defined by a dataset of context-target pairs, where both the context and target are sequences of tokens. For instance, in NL2SQL, the context is a natural language query and the target is the corresponding SQL command; in summarization, the context is an article's content and the target is its summary.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_search_query_tool], \n",
    "    \"What was mentioned about the problem statement in page 2?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The problem statement focuses on language modeling, particularly on maximizing conditional probabilities given a task-specific prompt. It involves adapting a pre-trained autoregressive language model to downstream conditional text generation tasks like summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each task is defined by a dataset of context-target pairs, where both the context and target are sequences of tokens. For instance, in NL2SQL, the context is a natural language query and the target is the corresponding SQL command; in summarization, the context is an article's content and the target is its summary.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\n",
      "depth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\n",
      "bardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\n",
      "match the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality.\n",
      "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\n",
      "over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\n",
      "change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\n",
      "Low-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\n",
      "network indirectly by optimizing rank decomposition matrices of the dense layers’ change during\n",
      "adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n",
      "175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufﬁces even\n",
      "when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.\n",
      "LoRA possesses several key advantages.\n",
      "• A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
      "ferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\n",
      "matricesAandBin Figure 1, reducing the storage requirement and task-switching over-\n",
      "head signiﬁcantly.\n",
      "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\n",
      "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
      "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
      "much smaller low-rank matrices.\n",
      "• Our simple linear design allows us to merge the trainable matrices with the frozen weights\n",
      "when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\n",
      "construction.\n",
      "• LoRA is orthogonal to many prior methods and can be combined with many of them, such\n",
      "as preﬁx-tuning. We provide an example in Appendix E.\n",
      "Terminologies and Conventions We make frequent references to the Transformer architecture\n",
      "and use the conventional terminologies for its dimensions. We call the input and output di-\n",
      "mension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\n",
      "query/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\n",
      "trained weight matrix and ∆Wits accumulated gradient update during adaptation. We use rto\n",
      "denote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\n",
      "Brown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\n",
      "optimization and use a Transformer MLP feedforward dimension dffn= 4×dmodel .\n",
      "2 P ROBLEM STATEMENT\n",
      "While our proposal is agnostic to training objective, we focus on language modeling as our motivat-\n",
      "ing use case. Below is a brief description of the language modeling problem and, in particular, the\n",
      "maximization of conditional probabilities given a task-speciﬁc prompt.\n",
      "Suppose we are given a pre-trained autoregressive language model PΦ(y|x)parametrized by Φ.\n",
      "For instance, PΦ(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\n",
      "et al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\n",
      "pre-trained model to downstream conditional text generation tasks, such as summarization, machine\n",
      "reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\n",
      "represented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\n",
      "yiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\n",
      "corresponding SQL command; for summarization, xiis the content of an article and yiits summary.\n",
      "2\n",
      "=============Text=============\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"=============Text=============\")\n",
    "    print(n.get_text())\n",
    "    print(\"=============Text=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to the Lora paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_search_tool with args: {\"query\": \"problem statement\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "The problem statement focuses on language modeling, particularly on maximizing conditional probabilities given a task-specific prompt. It involves adapting a pre-trained autoregressive language model to downstream conditional text generation tasks like summarization, machine reading comprehension (MRC), and natural language to SQL (NL2SQL). Each task is defined by a training dataset of context-target pairs, where both the context (xi) and target (yi) are sequences of tokens. For instance, in NL2SQL, xi represents a natural language query and yi its corresponding SQL command; in summarization, xi is the article content and yi is its summary.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_search_query_tool, summary_tool], \n",
    "    \"What was mentioned about the problem statement in page 2?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_search_tool with args: {\"query\": \"page 2\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "The proposed Low-RankAdaptation (LoRA) approach allows training some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers' change during adaptation while keeping the pre-trained weights frozen. This method is shown to be both storage- and compute-efficient, requiring very low rank matrices even when the full rank is high. Additionally, LoRA enables sharing a pre-trained model to build multiple small LoRA modules for different tasks, reducing storage requirements and task-switching overhead significantly. It also makes training more efficient by lowering the hardware barrier to entry and introduces no inference latency compared to a fully fine-tuned model when deployed.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_search_query_tool, summary_tool], \n",
    "    \"Give me a summary of what was mentioned in page 2?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"summary\"}\n",
      "=== Function Output ===\n",
      "LoRA is an adaptation strategy for large language models that involves freezing pre-trained model weights and adding trainable rank decomposition matrices to reduce the number of trainable parameters. Despite having fewer parameters, LoRA performs comparably or better than full fine-tuning on models like RoBERTa, DeBERTa, GPT-2, and GPT-3. It offers benefits such as reduced GPU memory usage, higher training speed, and no extra inference latency. The method allows for quick task-switching and can be integrated with PyTorch models. The paper discusses experiments on deep learning models, including adaptation methods, low-rank matrices, and subspace similarity measurements, providing insights into optimizing model performance and understanding deep learning dynamics.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_search_query_tool, summary_tool], \n",
    "    \"Give me a summary of the paper.\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '1', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "LORA: L OW\n",
      "=============Text=============\n",
      "{'page_label': '2', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "often intr\n",
      "=============Text=============\n",
      "{'page_label': '3', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "During ful\n",
      "=============Text=============\n",
      "{'page_label': '3', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "This makes\n",
      "=============Text=============\n",
      "{'page_label': '4', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Batch Size\n",
      "=============Text=============\n",
      "{'page_label': '4', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "LoRA takes\n",
      "=============Text=============\n",
      "{'page_label': '5', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "guarantees\n",
      "=============Text=============\n",
      "{'page_label': '5', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "See Append\n",
      "=============Text=============\n",
      "{'page_label': '6', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Model & Me\n",
      "=============Text=============\n",
      "{'page_label': '6', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "We report \n",
      "=============Text=============\n",
      "{'page_label': '7', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Model & Me\n",
      "=============Text=============\n",
      "{'page_label': '7', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "We also re\n",
      "=============Text=============\n",
      "{'page_label': '8', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Model&Meth\n",
      "=============Text=============\n",
      "{'page_label': '9', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "guage mode\n",
      "=============Text=============\n",
      "{'page_label': '9', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "In theory \n",
      "=============Text=============\n",
      "{'page_label': '10', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "to maximiz\n",
      "=============Text=============\n",
      "{'page_label': '10', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Table 6 sh\n",
      "=============Text=============\n",
      "{'page_label': '11', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Subspace s\n",
      "=============Text=============\n",
      "{'page_label': '12', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "0.00.10.20\n",
      "=============Text=============\n",
      "{'page_label': '13', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "tuning. 3)\n",
      "=============Text=============\n",
      "{'page_label': '13', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "160–167, N\n",
      "=============Text=============\n",
      "{'page_label': '14', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Behrooz Gh\n",
      "=============Text=============\n",
      "{'page_label': '14', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "arXiv: 180\n",
      "=============Text=============\n",
      "{'page_label': '15', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Xiao Liu, \n",
      "=============Text=============\n",
      "{'page_label': '15', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Learning m\n",
      "=============Text=============\n",
      "{'page_label': '16', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Ashish Vas\n",
      "=============Text=============\n",
      "{'page_label': '17', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Method MNL\n",
      "=============Text=============\n",
      "{'page_label': '18', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "and STS-B \n",
      "=============Text=============\n",
      "{'page_label': '19', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Method Dat\n",
      "=============Text=============\n",
      "{'page_label': '19', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Len. 128\n",
      "T\n",
      "=============Text=============\n",
      "{'page_label': '20', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Method Dat\n",
      "=============Text=============\n",
      "{'page_label': '21', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Hyperparam\n",
      "=============Text=============\n",
      "{'page_label': '22', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Method Web\n",
      "=============Text=============\n",
      "{'page_label': '23', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Method Hyp\n",
      "=============Text=============\n",
      "{'page_label': '23', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "MNLI-\n",
      "ndes\n",
      "=============Text=============\n",
      "{'page_label': '24', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "Hyperparam\n",
      "=============Text=============\n",
      "{'page_label': '25', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "0.00.20.40\n",
      "=============Text=============\n",
      "{'page_label': '26', 'file_name': 'lora_paper.pdf', 'file_path': 'datasets/lora_paper.pdf', 'file_type': 'application/pdf', 'file_size': 1609513, 'creation_date': '2024-05-10', 'last_modified_date': '2024-05-10'}\n",
      "=============Text=============\n",
      "0.00.10.20\n",
      "=============Text=============\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)\n",
    "    print(\"=============Text=============\")\n",
    "    print(n.get_text()[:10])\n",
    "    print(\"=============Text=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
